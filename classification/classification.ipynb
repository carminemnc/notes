{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler,OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datasage.styles import DARK_THEME,LIGHT_THEME\n",
    "from datasage.core import Leonardo\n",
    "leo = Leonardo()\n",
    "plt.style.use(LIGHT_THEME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/lorenzozoppelletto/financial-risk-for-loan-approval/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('loan.csv')\n",
    "# renaming columns for a better understanding\n",
    "df.columns = [re.sub( '(?<!^)(?=[A-Z])', '_', x).lower() for x in df.columns.to_list()]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leo.binary_ratio_plot(data=df,column_name='loan_approved',target_zero_name=\"Approved\",target_one_name=\"Not approved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='loan_approved', y='risk_score', data=df)\n",
    "plt.title('Distribuzione del Risk Score per Prestiti Approvati/Non Approvati')\n",
    "plt.xlabel('Prestito Approvato (0=No, 1=SÃ¬)')\n",
    "plt.ylabel('Risk Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop target column\n",
    "_v = df.drop(columns='attrition')\n",
    "# categorical columns list\n",
    "cat = _v.select_dtypes(include=['object']).columns.to_list()\n",
    "# numerical columns list\n",
    "num = _v.loc[:, ~_v.columns.isin(cat)].columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent features, target feature\n",
    "x,y = df.drop(columns='attrition'),df.attrition.map({'Yes': 1, 'No': 0})\n",
    "# train/test split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for numerical features\n",
    "num_pipe = Pipeline(\n",
    "    [\n",
    "        ('scaler', RobustScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# pipeline for categorical features\n",
    "cat_pipe = Pipeline(\n",
    "    [\n",
    "        ('encoder', OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# column transformer for applying transformations only on some columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('Numerical features',num_pipe,num),\n",
    "        ('Categorical features',cat_pipe,cat)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = LogisticRegression()\n",
    " \n",
    "# entire pipeline\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('Preprocessor',preprocessor),\n",
    "        (\"Model\", model)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_diagnostic(model,x_train,y_train,x_test,y_test):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns different metrics for a binary classification task.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    \n",
    "    model: `a model object`\n",
    "        Pandas dataframe object.\n",
    "    x_train,y_train,x_test,y_test: `str`\n",
    "        The dataframe column's name on which extracting features.\n",
    "\n",
    "    Returns:\n",
    "    data: `pandas dataframe object`\n",
    "        Pandas dataframe with additional features.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # fitting the model\n",
    "    model.fit(x_train,y_train) \n",
    "    # predictions\n",
    "    preds = pipe.predict(x_test)\n",
    "    # predicted probabilities\n",
    "    pred_probs = pipe.predict_proba(x_test)[:,1]    \n",
    "    \n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test,preds)\n",
    "    # roc curve\n",
    "    fpr,tpr,_ = roc_curve(y_test,pred_probs)\n",
    "    # precision-recall curve\n",
    "    pre,rec,_ = precision_recall_curve(y_test,pred_probs)\n",
    "    # classification report\n",
    "    clf_report = classification_report(y_test,preds,output_dict=True)\n",
    "    # true negatives, false positives, false negatives, true positives\n",
    "    tn,fp,fn,tp = cm.ravel()\n",
    "    \n",
    "    m = {\n",
    "    'Accuracy': (tp + tn) / (tp + fn + tn + fn),\n",
    "    'Type I error': fp/(fp + tn),\n",
    "    'Type II error': fn/(tp + fn),\n",
    "    'Specificity': tn / (tn + fp),\n",
    "    'Recall': tp / (tp + fn),\n",
    "    'Precision': tp/ (tp + fp),\n",
    "    'F0.5': fbeta_score(y_test,preds,beta=0.5),\n",
    "    'F1': fbeta_score(y_test,preds,beta=1),\n",
    "    'F2': fbeta_score(y_test,preds,beta=2),\n",
    "    'MCC': matthews_corrcoef(y_test,preds),\n",
    "    'Negative predictive value': tn / (tn + fn),\n",
    "    'False discovery rate': fp / (fp + tp),\n",
    "    'Log loss': log_loss(y_test,preds),\n",
    "    'Average precision score': average_precision_score(y_test,preds),\n",
    "    'AUC': roc_auc_score(y_test,preds)\n",
    "    }\n",
    "    \n",
    "    metrics = pd.DataFrame([m],index=['Value']).T\n",
    "    \n",
    "    # plots layout\n",
    "    fig, ax = plt.subplot_mosaic([['A', 'B','C'],['D', 'D','E']],\n",
    "                                 constrained_layout=True,\n",
    "                                 figsize=(13,8))\n",
    "    \n",
    "    # confusion matrix plot\n",
    "    sns.heatmap(cm,annot=True,ax=ax['A'],fmt='.3g',cbar=False)\n",
    "    ax['A'].set_title('Confusion matrix')\n",
    "    ax['A'].set_xlabel('Predicted classes')\n",
    "    ax['A'].set_ylabel('Actual classes')\n",
    "    \n",
    "    # metrics plot\n",
    "    sns.heatmap(metrics,annot=True,ax=ax['B'],fmt='.3f',cbar=False)\n",
    "    ax['B'].xaxis.tick_top() # x axis on top\n",
    "    ax['B'].set_title('Metrics')\n",
    "\n",
    "    # classification report plot\n",
    "    sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True,ax=ax['C'],cbar=False,fmt='.4f')\n",
    "    ax['C'].xaxis.tick_top() # x axis on top\n",
    "    ax['C'].set_title('Classification report')\n",
    "    \n",
    "    # Precision-recall curve\n",
    "    ax['D'].plot(pre,rec)\n",
    "    ax['D'].set_title('Precision-recall curve')\n",
    "    ax['D'].set_xlabel('Precision')\n",
    "    ax['D'].set_ylabel('Recall')\n",
    "    \n",
    "    # ROC Curve\n",
    "    ax['E'].plot(fpr,tpr)\n",
    "    ax['E'].set_title('ROC curve')\n",
    "    ax['E'].plot([0,1],[0,1],linestyle='dashed',color='red')\n",
    "    ax['E'].set_xlabel('FPR')\n",
    "    ax['E'].set_ylabel('TPR')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "classifier_diagnostic(pipe,x_train,y_train,x_test,y_test)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity sake we're gonna rename:\n",
    "\n",
    "- $True \\ negatives \\ = tn$\n",
    "- $True \\ positives \\ = tp$\n",
    "- $False \\ positives \\ = fp$\n",
    "- $False \\ negatives \\ = fn$\n",
    " \n",
    "#### Accuracy\n",
    "$$\n",
    "Accuracy = \\frac{tn + tp}{tn + tp + fp + fn}\n",
    "$$\n",
    "\n",
    "It measures how many observations, both positive and negative, were correctly classified.\n",
    "\n",
    "#### Type I error\n",
    "$$\n",
    "Type \\ I \\ error = \\frac{fp}{fp + tn}\n",
    "$$\n",
    "\n",
    "When the model predict \"1\" on a ground truth of \"0\" this is identified as Type I error.\n",
    "<!-- When the cost of **\"false alerts\"** is high, we would want to have a low value for this metric. -->\n",
    "\n",
    "#### Type II error\n",
    "$$\n",
    "Type \\ I \\ error = \\frac{fn}{tp + fn}\n",
    "$$\n",
    "\n",
    "When the model predict \"0\" on a ground truth of \"1\" this is identified as Type II error.\n",
    "\n",
    "#### Specificity\n",
    "$$\n",
    "Specificity = \\frac{tn}{tn + fp}\n",
    "$$\n",
    "\n",
    "How many \"0\"s the model predict out of all \"0\" classes.\n",
    "We would expect that on heavily unbalanced problem the value of $Specificity$ will be really high.\n",
    "\n",
    "#### Recall or Sensitivity\n",
    "$$\n",
    "Recall = \\frac{tp}{tp + fn}\n",
    "$$\n",
    "\n",
    "How many \"1\"s the model predict out of all \"1\" classes.\n",
    "Technically maximizing $Recall$ metric would mean trying to minimize the number of **false negatives** \n",
    "\n",
    "\n",
    "#### Precision\n",
    "$$\n",
    "Precision \\ = \\frac{tp}{tp + fp}\n",
    "$$\n",
    "\n",
    "How many \"1\"s the model predict are in fact \"1\".\n",
    "Technically maximizing $Precision$ metric would mean trying to minimize the number of **false positives** aka *false alerts*.\n",
    "\n",
    "#### F beta score\n",
    "$$\n",
    "F_{beta} = (1 + \\beta^{2})\\frac{Precision * Recall}{(\\beta^{2}*precision) + recall}\n",
    "$$\n",
    "\n",
    "The value of $\\beta$ in our $F_{beta}$ score will weight how much we care about $Recall$ or $Precision$.\n",
    "\n",
    "- $0<\\beta \\le 1$ gives more weight to $Precision$ over $Recall$, meaning that we care more about minimizing *false negatives*. \n",
    "- $1<\\beta \\le 2$ gives more weight to $Recall$ over $Precision$, meaning that we care more about minimizing *false alerts*.\n",
    "\n",
    "#### Matthews correlation coefficient ($MCC$)\n",
    "$$\n",
    "MCC = \\frac{(tp*tn) - (fp*fn)}{(tp + fp)(tp+fn)(tn+fp)(tn+fn)}\n",
    "$$\n",
    "\n",
    "It measures the correlation between predicted classes and ground truth.\n",
    "\n",
    "\n",
    "#### Average precision\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
